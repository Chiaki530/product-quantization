# load the base data ../data/sift100m/sift100m_base.fvecs, 
# load the queries ../data/sift100m/sift100m_query.fvecs, 
# load the ground truth ../data/sift100m/20_sift100m_product_groundtruth.ivecs
# ranking metric product
# Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 0,  residual average norm : 271.045654296875 max norm: 435.131103515625 min norm: 69.10592651367188
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 1,  residual average norm : 234.2046661376953 max norm: 367.9516296386719 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 2,  residual average norm : 211.5129852294922 max norm: 346.05670166015625 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 3,  residual average norm : 194.90513610839844 max norm: 314.37835693359375 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 4,  residual average norm : 181.8280792236328 max norm: 300.7534484863281 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 5,  residual average norm : 171.2321014404297 max norm: 286.0918884277344 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 6,  residual average norm : 161.97889709472656 max norm: 267.44781494140625 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 7,  residual average norm : 153.9552001953125 max norm: 256.2215576171875 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 8,  residual average norm : 146.7962646484375 max norm: 248.63694763183594 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 9,  residual average norm : 140.50790405273438 max norm: 240.27944946289062 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 10,  residual average norm : 134.91647338867188 max norm: 230.2484588623047 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 11,  residual average norm : 129.59683227539062 max norm: 220.99850463867188 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 12,  residual average norm : 124.67475128173828 max norm: 211.39083862304688 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 13,  residual average norm : 120.10186767578125 max norm: 204.2615203857422 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 14,  residual average norm : 115.95976257324219 max norm: 200.23768615722656 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 15,  residual average norm : 112.01680755615234 max norm: 193.53945922851562 min norm: 0.0
# compress items
# sorting items
# searching!
expected items, overall time, avg recall, avg precision, avg error, avg items
1, 0, 0.0215, 0.42999999999999994, 0, 1
2, 0, 0.037399999999999996, 0.37399999999999994, 0, 2
4, 0, 0.06365000000000001, 0.31825000000000003, 0, 4
8, 0, 0.10635, 0.265875, 0, 8
16, 0, 0.16775, 0.20968750000000003, 0, 16
32, 0, 0.2496, 0.156, 0, 32
64, 0, 0.35364999999999996, 0.11051562499999999, 0, 64
128, 0, 0.4741, 0.07407812500000001, 0, 128
256, 0, 0.5982999999999999, 0.0467421875, 0, 256
512, 0, 0.7178499999999999, 0.028041015624999995, 0, 512
1024, 0, 0.82155, 0.0160458984375, 0, 1024
2048, 0, 0.89535, 0.00874365234375, 0, 2048
4096, 0, 0.9441, 0.00460986328125, 0, 4096
8192, 0, 0.9713000000000002, 0.0023713378906250002, 0, 8192
16384, 0, 0.9861500000000001, 0.0012037963867187502, 0, 16384
32768, 0, 0.99425, 0.0006068420410156249, 0, 32768
65536, 0, 0.9982000000000001, 0.00030462646484375003, 0, 65536
131072, 0, 0.9994999999999998, 0.00015251159667968746, 0, 131072
262144, 0, 0.9994999999999998, 7.625579833984373e-05, 0, 262144
524288, 0, 0.9994999999999998, 3.8127899169921865e-05, 0, 524288
1048576, 0, 0.9994999999999998, 1.9063949584960933e-05, 0, 1048576
2097152, 0, 0.9994999999999998, 9.531974792480466e-06, 0, 2097152
4194304, 0, 0.9994999999999998, 4.765987396240233e-06, 0, 4194304
8388608, 0, 0.9994999999999998, 2.3829936981201166e-06, 0, 8388608
16777216, 0, 0.9994999999999998, 1.1914968490600583e-06, 0, 16777216
33554432, 0, 0.9994999999999998, 5.957484245300291e-07, 0, 33554432
67108864, 0, 0.9994999999999998, 2.9787421226501457e-07, 0, 67108864
134217728, 0, 0.9994999999999998, 1.4893710613250729e-07, 0, 134217728

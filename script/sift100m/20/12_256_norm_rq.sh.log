# load the base data ../data/sift100m/sift100m_base.fvecs, 
# load the queries ../data/sift100m/sift100m_query.fvecs, 
# load the ground truth ../data/sift100m/20_sift100m_product_groundtruth.ivecs
# ranking metric product
# NormPQ, percentiles: 256, quantize: Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 0,  residual average norm : 0.532881498336792 max norm: 0.8555207252502441 min norm: 0.13526023924350739
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 1,  residual average norm : 0.4604681730270386 max norm: 0.7214657664299011 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 2,  residual average norm : 0.41579264402389526 max norm: 0.6612787842750549 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 3,  residual average norm : 0.3834145665168762 max norm: 0.6246591806411743 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 4,  residual average norm : 0.357653945684433 max norm: 0.5949466228485107 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 5,  residual average norm : 0.336703896522522 max norm: 0.5655081868171692 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 6,  residual average norm : 0.31853803992271423 max norm: 0.5331188440322876 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 7,  residual average norm : 0.302677720785141 max norm: 0.5152149200439453 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 8,  residual average norm : 0.2883448302745819 max norm: 0.49927762150764465 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 9,  residual average norm : 0.2762560248374939 max norm: 0.4845094680786133 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 10,  residual average norm : 0.2649361193180084 max norm: 0.45839154720306396 min norm: 0.0
# compress items
# sorting items
# searching!
expected items, overall time, avg recall, avg precision, avg error, avg items
1, 0, 0.033949999999999994, 0.6789999999999998, 0, 1
2, 0, 0.06315, 0.6315, 0, 2
4, 0, 0.11394999999999998, 0.5697499999999999, 0, 4
8, 0, 0.19335000000000002, 0.48337500000000005, 0, 8
16, 0, 0.30965, 0.3870625, 0, 16
32, 0, 0.45330000000000004, 0.2833125, 0, 32
64, 0, 0.61165, 0.191140625, 0, 64
128, 0, 0.7541, 0.117828125, 0, 128
256, 0, 0.8677500000000001, 0.06779296875000002, 0, 256
512, 0, 0.9378, 0.0366328125, 0, 512
1024, 0, 0.9758000000000001, 0.019058593750000002, 0, 1024
2048, 0, 0.9912500000000001, 0.009680175781250001, 0, 2048
4096, 0, 0.9974999999999998, 0.004870605468749999, 0, 4096
8192, 0, 0.99935, 0.0024398193359375, 0, 8192
16384, 0, 0.9999, 0.0012205810546875, 0, 16384
32768, 0, 1.0, 0.0006103515625, 0, 32768
65536, 0, 1.0, 0.00030517578125, 0, 65536
131072, 0, 1.0, 0.000152587890625, 0, 131072
262144, 0, 1.0, 7.62939453125e-05, 0, 262144
524288, 0, 1.0, 3.814697265625e-05, 0, 524288
1048576, 0, 1.0, 1.9073486328125e-05, 0, 1048576
2097152, 0, 1.0, 9.5367431640625e-06, 0, 2097152
4194304, 0, 1.0, 4.76837158203125e-06, 0, 4194304
8388608, 0, 1.0, 2.384185791015625e-06, 0, 8388608
16777216, 0, 1.0, 1.1920928955078125e-06, 0, 16777216
33554432, 0, 1.0, 5.960464477539062e-07, 0, 33554432
67108864, 0, 1.0, 2.980232238769531e-07, 0, 67108864
134217728, 0, 1.0, 1.4901161193847656e-07, 0, 134217728

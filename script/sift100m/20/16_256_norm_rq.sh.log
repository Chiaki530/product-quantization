# load the base data ../data/sift100m/sift100m_base.fvecs, 
# load the queries ../data/sift100m/sift100m_query.fvecs, 
# load the ground truth ../data/sift100m/20_sift100m_product_groundtruth.ivecs
# ranking metric product
# NormPQ, percentiles: 256, quantize: Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 0,  residual average norm : 0.5328831672668457 max norm: 0.8545288443565369 min norm: 0.1351248323917389
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 1,  residual average norm : 0.46043261885643005 max norm: 0.722808837890625 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 2,  residual average norm : 0.4157136380672455 max norm: 0.660210132598877 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 3,  residual average norm : 0.38339948654174805 max norm: 0.6239116191864014 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 4,  residual average norm : 0.3576872646808624 max norm: 0.5958994626998901 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 5,  residual average norm : 0.3367376923561096 max norm: 0.5708221793174744 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 6,  residual average norm : 0.318513423204422 max norm: 0.5420652627944946 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 7,  residual average norm : 0.3026094436645508 max norm: 0.5208545327186584 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 8,  residual average norm : 0.28842055797576904 max norm: 0.498018354177475 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 9,  residual average norm : 0.27595776319503784 max norm: 0.4809493124485016 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 10,  residual average norm : 0.26497331261634827 max norm: 0.4656788408756256 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 11,  residual average norm : 0.2546531558036804 max norm: 0.4545477032661438 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 12,  residual average norm : 0.2451905459165573 max norm: 0.4423125088214874 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 13,  residual average norm : 0.23668541014194489 max norm: 0.4254593253135681 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 14,  residual average norm : 0.22910088300704956 max norm: 0.4100385308265686 min norm: 0.0
# compress items
# sorting items
# searching!
expected items, overall time, avg recall, avg precision, avg error, avg items
1, 0, 0.04159999999999999, 0.8319999999999999, 0, 1
2, 0, 0.07995000000000001, 0.7995000000000001, 0, 2
4, 0, 0.14579999999999999, 0.7289999999999999, 0, 4
8, 0, 0.2533, 0.6332500000000001, 0, 8
16, 0, 0.40169999999999995, 0.5021249999999999, 0, 16
32, 0, 0.5801999999999999, 0.362625, 0, 32
64, 0, 0.7508, 0.234625, 0, 64
128, 0, 0.8719499999999998, 0.13624218749999997, 0, 128
256, 0, 0.9493, 0.0741640625, 0, 256
512, 0, 0.9825999999999999, 0.038382812499999995, 0, 512
1024, 0, 0.9950500000000001, 0.019434570312500003, 0, 1024
2048, 0, 0.99905, 0.00975634765625, 0, 2048
4096, 0, 0.9997999999999999, 0.0048818359375, 0, 4096
8192, 0, 0.99995, 0.0024412841796875, 0, 8192
16384, 0, 0.99995, 0.00122064208984375, 0, 16384
32768, 0, 1.0, 0.0006103515625, 0, 32768
65536, 0, 1.0, 0.00030517578125, 0, 65536
131072, 0, 1.0, 0.000152587890625, 0, 131072
262144, 0, 1.0, 7.62939453125e-05, 0, 262144
524288, 0, 1.0, 3.814697265625e-05, 0, 524288
1048576, 0, 1.0, 1.9073486328125e-05, 0, 1048576
2097152, 0, 1.0, 9.5367431640625e-06, 0, 2097152
4194304, 0, 1.0, 4.76837158203125e-06, 0, 4194304
8388608, 0, 1.0, 2.384185791015625e-06, 0, 8388608
16777216, 0, 1.0, 1.1920928955078125e-06, 0, 16777216
33554432, 0, 1.0, 5.960464477539062e-07, 0, 33554432
67108864, 0, 1.0, 2.980232238769531e-07, 0, 67108864
134217728, 0, 1.0, 1.4901161193847656e-07, 0, 134217728

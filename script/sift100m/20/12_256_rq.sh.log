# load the base data ../data/sift100m/sift100m_base.fvecs, 
# load the queries ../data/sift100m/sift100m_query.fvecs, 
# load the ground truth ../data/sift100m/20_sift100m_product_groundtruth.ivecs
# ranking metric product
# Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 0,  residual average norm : 271.0462951660156 max norm: 435.2031555175781 min norm: 69.10726928710938
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 1,  residual average norm : 234.21827697753906 max norm: 367.3070068359375 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 2,  residual average norm : 211.5290985107422 max norm: 347.0196838378906 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 3,  residual average norm : 195.01083374023438 max norm: 324.4926452636719 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 4,  residual average norm : 181.9425811767578 max norm: 310.5679931640625 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 5,  residual average norm : 171.24537658691406 max norm: 293.8828430175781 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 6,  residual average norm : 162.08192443847656 max norm: 283.1855773925781 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 7,  residual average norm : 154.08941650390625 max norm: 263.30206298828125 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 8,  residual average norm : 146.8446044921875 max norm: 251.05010986328125 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 9,  residual average norm : 140.60020446777344 max norm: 242.16233825683594 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 10,  residual average norm : 134.9667510986328 max norm: 226.80772399902344 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 128
# layer: 11,  residual average norm : 129.6446990966797 max norm: 219.2615509033203 min norm: 0.0
# compress items
# sorting items
# searching!
expected items, overall time, avg recall, avg precision, avg error, avg items
1, 0, 0.01365, 0.273, 0, 1
2, 0, 0.02375, 0.2375, 0, 2
4, 0, 0.03875, 0.19375, 0, 4
8, 0, 0.06675, 0.166875, 0, 8
16, 0, 0.10799999999999998, 0.13499999999999998, 0, 16
32, 0, 0.16560000000000002, 0.10350000000000001, 0, 32
64, 0, 0.24109999999999998, 0.07534374999999999, 0, 64
128, 0, 0.33825, 0.0528515625, 0, 128
256, 0, 0.4497, 0.0351328125, 0, 256
512, 0, 0.5679000000000001, 0.02218359375, 0, 512
1024, 0, 0.6877000000000001, 0.013431640625000001, 0, 1024
2048, 0, 0.7878999999999999, 0.0076943359375, 0, 2048
4096, 0, 0.8702, 0.0042490234375, 0, 4096
8192, 0, 0.9283500000000001, 0.0022664794921875005, 0, 8192
16384, 0, 0.96465, 0.00117755126953125, 0, 16384
32768, 0, 0.9839499999999999, 0.000600555419921875, 0, 32768
65536, 0, 0.9950000000000001, 0.00030364990234375003, 0, 65536
131072, 0, 0.9986499999999999, 0.00015238189697265624, 0, 131072
262144, 0, 0.9986499999999999, 7.619094848632812e-05, 0, 262144
524288, 0, 0.9986499999999999, 3.809547424316406e-05, 0, 524288
1048576, 0, 0.9986499999999999, 1.904773712158203e-05, 0, 1048576
2097152, 0, 0.9986499999999999, 9.523868560791015e-06, 0, 2097152
4194304, 0, 0.9986499999999999, 4.7619342803955076e-06, 0, 4194304
8388608, 0, 0.9986499999999999, 2.3809671401977538e-06, 0, 8388608
16777216, 0, 0.9986499999999999, 1.1904835700988769e-06, 0, 16777216
33554432, 0, 0.9986499999999999, 5.952417850494384e-07, 0, 33554432
67108864, 0, 0.9986499999999999, 2.976208925247192e-07, 0, 67108864
134217728, 0, 0.9986499999999999, 1.488104462623596e-07, 0, 134217728

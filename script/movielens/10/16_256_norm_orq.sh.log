# load the base data ../data/movielens/movielens_base.fvecs, 
# load the queries ../data/movielens/movielens_query.fvecs, 
# load the ground truth ../data/movielens/10_movielens_product_groundtruth.ivecs
# ranking metric product
# NormPQ, percentiles: 256, quantize: ORQ, RQ : [Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>],  M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 0,  residual average norm : 0.2589826285839081 max norm: 0.8725853562355042 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 1,  residual average norm : 0.19009409844875336 max norm: 0.6136480569839478 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 2,  residual average norm : 0.14743638038635254 max norm: 0.5187203288078308 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 3,  residual average norm : 0.11780887097120285 max norm: 0.4123508632183075 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 4,  residual average norm : 0.09587177634239197 max norm: 0.34496361017227173 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 5,  residual average norm : 0.07940060645341873 max norm: 0.3175742030143738 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 6,  residual average norm : 0.06669648736715317 max norm: 0.21360838413238525 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 7,  residual average norm : 0.05680153891444206 max norm: 0.18938027322292328 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 8,  residual average norm : 0.04885048791766167 max norm: 0.1726897805929184 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 9,  residual average norm : 0.04225801303982735 max norm: 0.15187223255634308 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 10,  residual average norm : 0.036929190158843994 max norm: 0.10352178663015366 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 11,  residual average norm : 0.032552722841501236 max norm: 0.08769315481185913 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 12,  residual average norm : 0.02885531447827816 max norm: 0.07674302905797958 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 13,  residual average norm : 0.025697117671370506 max norm: 0.06920009106397629 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 14,  residual average norm : 0.023038066923618317 max norm: 0.06577994674444199 min norm: 0.0
# compress items
# sorting items
# searching!
expected items, overall time, avg recall, avg precision, avg error, avg items
1, 0, 0.1, 1.0, 0, 1
2, 0, 0.1998, 0.999, 0, 2
4, 0, 0.39580000000000004, 0.9895, 0, 4
8, 0, 0.7493000000000001, 0.936625, 0, 8
16, 0, 0.9686999999999999, 0.6054375, 0, 16
32, 0, 0.9923, 0.31009375, 0, 32
64, 0, 0.9953, 0.155515625, 0, 64
128, 0, 0.9977, 0.0779453125, 0, 128
256, 0, 0.9997999999999999, 0.0390546875, 0, 256
512, 0, 1.0, 0.01953125, 0, 512
1024, 0, 1.0, 0.009765625, 0, 1024
2048, 0, 1.0, 0.0048828125, 0, 2048
4096, 0, 1.0, 0.00244140625, 0, 4096
8192, 0, 1.0, 0.001220703125, 0, 8192
16384, 0, 1.0, 0.0009365926758452749, 0, 10677

# load the base data ../data/yahoomusic/yahoomusic_base.fvecs, 
# load the queries ../data/yahoomusic/yahoomusic_query.fvecs, 
# load the ground truth ../data/yahoomusic/50_yahoomusic_product_groundtruth.ivecs
# ranking metric product
# NormPQ, percentiles: 256, quantize: Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 0,  residual average norm : 0.31513598561286926 max norm: 0.8053091764450073 min norm: 0.08149206638336182
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 1,  residual average norm : 0.2417791336774826 max norm: 0.7417517900466919 min norm: 0.0414564423263073
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 2,  residual average norm : 0.20153555274009705 max norm: 0.7147493362426758 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 3,  residual average norm : 0.17354628443717957 max norm: 0.6515963673591614 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 4,  residual average norm : 0.15218861401081085 max norm: 0.629899799823761 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 5,  residual average norm : 0.13503216207027435 max norm: 0.5069466829299927 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 6,  residual average norm : 0.12084200233221054 max norm: 0.4863525629043579 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 7,  residual average norm : 0.10892261564731598 max norm: 0.4701666831970215 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 8,  residual average norm : 0.09878700226545334 max norm: 0.3996625244617462 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 9,  residual average norm : 0.09003226459026337 max norm: 0.3249395489692688 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 10,  residual average norm : 0.08237892389297485 max norm: 0.3087674379348755 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 11,  residual average norm : 0.07564156502485275 max norm: 0.29734325408935547 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 12,  residual average norm : 0.06977227330207825 max norm: 0.28316712379455566 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 13,  residual average norm : 0.06452936679124832 max norm: 0.26464441418647766 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 14,  residual average norm : 0.05983295291662216 max norm: 0.25440749526023865 min norm: 0.0
# compress items
# sorting items
# searching!
expected items, overall time, avg recall, avg precision, avg error, avg items
1, 0, 0.01996, 0.9979999999999999, 0, 1
2, 0, 0.039900000000000005, 0.9975000000000002, 0, 2
4, 0, 0.07978, 0.9972500000000001, 0, 4
8, 0, 0.15926, 0.9953750000000001, 0, 8
16, 0, 0.31670000000000004, 0.9896875000000002, 0, 16
32, 0, 0.60658, 0.94778125, 0, 32
64, 0, 0.92054, 0.719171875, 0, 64
128, 0, 0.9842799999999999, 0.384484375, 0, 128
256, 0, 0.99804, 0.1949296875, 0, 256
512, 0, 0.99992, 0.0976484375, 0, 512
1024, 0, 0.9999800000000001, 0.0488271484375, 0, 1024
2048, 0, 1.0, 0.0244140625, 0, 2048
4096, 0, 1.0, 0.01220703125, 0, 4096
8192, 0, 1.0, 0.006103515625, 0, 8192
16384, 0, 1.0, 0.0030517578125, 0, 16384
32768, 0, 1.0, 0.00152587890625, 0, 32768
65536, 0, 1.0, 0.000762939453125, 0, 65536
131072, 0, 1.0, 0.0003814697265625, 0, 131072
262144, 0, 1.0, 0.0003656681488415633, 0, 136736

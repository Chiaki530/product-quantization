# load the base data ../data/netflix/netflix_base.fvecs, 
# load the queries ../data/netflix/netflix_query.fvecs, 
# load the ground truth ../data/netflix/1_netflix_product_groundtruth.ivecs
# ranking metric product
# Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 0,  residual average norm : 0.4127624034881592 max norm: 1.2760446071624756 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 1,  residual average norm : 0.2917933762073517 max norm: 1.0379552841186523 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 2,  residual average norm : 0.22355420887470245 max norm: 0.8585641384124756 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 3,  residual average norm : 0.1775599867105484 max norm: 0.6975325345993042 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 4,  residual average norm : 0.14493897557258606 max norm: 0.580616295337677 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 5,  residual average norm : 0.12056272476911545 max norm: 0.4838661253452301 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 6,  residual average norm : 0.10214150696992874 max norm: 0.4166892468929291 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 7,  residual average norm : 0.0879243016242981 max norm: 0.3760587275028229 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 8,  residual average norm : 0.07639732956886292 max norm: 0.3194526731967926 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 9,  residual average norm : 0.067112497985363 max norm: 0.27167823910713196 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 10,  residual average norm : 0.05949116125702858 max norm: 0.25928995013237 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 11,  residual average norm : 0.053204238414764404 max norm: 0.19404226541519165 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 12,  residual average norm : 0.04794115200638771 max norm: 0.17704367637634277 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 13,  residual average norm : 0.043492116034030914 max norm: 0.1526803970336914 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 14,  residual average norm : 0.03967352584004402 max norm: 0.14501893520355225 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 300
# layer: 15,  residual average norm : 0.0363922156393528 max norm: 0.12152978777885437 min norm: 0.0
# compress items
# sorting items
# searching!
expected items, overall time, avg recall, avg precision, avg error, avg items
1, 0, 0.753, 0.753, 0, 1
2, 0, 0.952, 0.476, 0, 2
4, 0, 0.998, 0.2495, 0, 4
8, 0, 1.0, 0.125, 0, 8
16, 0, 1.0, 0.0625, 0, 16
32, 0, 1.0, 0.03125, 0, 32
64, 0, 1.0, 0.015625, 0, 64
128, 0, 1.0, 0.0078125, 0, 128
256, 0, 1.0, 0.00390625, 0, 256
512, 0, 1.0, 0.001953125, 0, 512
1024, 0, 1.0, 0.0009765625, 0, 1024
2048, 0, 1.0, 0.00048828125, 0, 2048
4096, 0, 1.0, 0.000244140625, 0, 4096
8192, 0, 1.0, 0.0001220703125, 0, 8192
16384, 0, 1.0, 6.103515625e-05, 0, 16384
32768, 0, 1.0, 5.627462014631401e-05, 0, 17770

# load the base data ../data/imagenet/imagenet_base.fvecs, 
# load the queries ../data/imagenet/imagenet_query.fvecs, 
# load the ground truth ../data/imagenet/1_imagenet_product_groundtruth.ivecs
# ranking metric product
# NormPQ, percentiles: 256, quantize: Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 0,  residual average norm : 0.6843136548995972 max norm: 1.0400437116622925 min norm: 0.13191361725330353
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 1,  residual average norm : 0.6002913117408752 max norm: 0.9870588183403015 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 2,  residual average norm : 0.5481430292129517 max norm: 0.9561288356781006 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 3,  residual average norm : 0.5101339817047119 max norm: 0.9098238945007324 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 4,  residual average norm : 0.4790351688861847 max norm: 0.8674002885818481 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 5,  residual average norm : 0.45343542098999023 max norm: 0.8353294134140015 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 6,  residual average norm : 0.43116191029548645 max norm: 0.8188182711601257 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 7,  residual average norm : 0.41172903776168823 max norm: 0.784500241279602 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 8,  residual average norm : 0.3933652341365814 max norm: 0.7633950710296631 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 9,  residual average norm : 0.37762272357940674 max norm: 0.7402758598327637 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 10,  residual average norm : 0.3625807464122772 max norm: 0.6986029148101807 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 11,  residual average norm : 0.34858238697052 max norm: 0.6869634985923767 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 12,  residual average norm : 0.3363465666770935 max norm: 0.6690686941146851 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 13,  residual average norm : 0.325091689825058 max norm: 0.6541786193847656 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 14,  residual average norm : 0.3141559064388275 max norm: 0.6393654942512512 min norm: 0.0
# compress items
# sorting items
# searching!
expected items, overall time, avg recall, avg precision, avg error, avg items
1, 0, 0.321, 0.321, 0, 1
2, 0, 0.443, 0.2215, 0, 2
4, 0, 0.587, 0.14675, 0, 4
8, 0, 0.731, 0.091375, 0, 8
16, 0, 0.837, 0.0523125, 0, 16
32, 0, 0.915, 0.02859375, 0, 32
64, 0, 0.962, 0.01503125, 0, 64
128, 0, 0.987, 0.0077109375, 0, 128
256, 0, 0.993, 0.00387890625, 0, 256
512, 0, 0.999, 0.001951171875, 0, 512
1024, 0, 1.0, 0.0009765625, 0, 1024
2048, 0, 1.0, 0.00048828125, 0, 2048
4096, 0, 1.0, 0.000244140625, 0, 4096
8192, 0, 1.0, 0.0001220703125, 0, 8192
16384, 0, 1.0, 6.103515625e-05, 0, 16384
32768, 0, 1.0, 3.0517578125e-05, 0, 32768
65536, 0, 1.0, 1.52587890625e-05, 0, 65536
131072, 0, 1.0, 7.62939453125e-06, 0, 131072
262144, 0, 1.0, 3.814697265625e-06, 0, 262144
524288, 0, 1.0, 1.9073486328125e-06, 0, 524288
1048576, 0, 1.0, 9.5367431640625e-07, 0, 1048576
2097152, 0, 1.0, 4.76837158203125e-07, 0, 2097152
4194304, 0, 1.0, 4.272823178185699e-07, 0, 2340373

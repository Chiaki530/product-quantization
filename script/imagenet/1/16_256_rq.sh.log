# load the base data ../data/imagenet/imagenet_base.fvecs, 
# load the queries ../data/imagenet/imagenet_query.fvecs, 
# load the ground truth ../data/imagenet/1_imagenet_product_groundtruth.ivecs
# ranking metric product
# Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 0,  residual average norm : 0.20509226620197296 max norm: 0.8650335669517517 min norm: 0.028300635516643524
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 1,  residual average norm : 0.17940162122249603 max norm: 0.8014782667160034 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 2,  residual average norm : 0.16379638016223907 max norm: 0.6748961210250854 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 3,  residual average norm : 0.15229861438274384 max norm: 0.5869590640068054 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 4,  residual average norm : 0.1430693417787552 max norm: 0.560660719871521 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 5,  residual average norm : 0.13535363972187042 max norm: 0.5163114070892334 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 6,  residual average norm : 0.12869217991828918 max norm: 0.45330479741096497 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 7,  residual average norm : 0.12286444008350372 max norm: 0.43472933769226074 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 8,  residual average norm : 0.1176186054944992 max norm: 0.4265536367893219 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 9,  residual average norm : 0.11278467625379562 max norm: 0.3751920759677887 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 10,  residual average norm : 0.10829563438892365 max norm: 0.35333606600761414 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 11,  residual average norm : 0.1043010950088501 max norm: 0.31706005334854126 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 12,  residual average norm : 0.10055536776781082 max norm: 0.3094043731689453 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 13,  residual average norm : 0.09706393629312515 max norm: 0.2864443361759186 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 14,  residual average norm : 0.09373613446950912 max norm: 0.2762368619441986 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 15,  residual average norm : 0.09060989320278168 max norm: 0.26628074049949646 min norm: 0.0
# compress items
# sorting items
# searching!
expected items, overall time, avg recall, avg precision, avg error, avg items
1, 0, 0.314, 0.314, 0, 1
2, 0, 0.474, 0.237, 0, 2
4, 0, 0.611, 0.15275, 0, 4
8, 0, 0.757, 0.094625, 0, 8
16, 0, 0.851, 0.0531875, 0, 16
32, 0, 0.902, 0.0281875, 0, 32
64, 0, 0.954, 0.01490625, 0, 64
128, 0, 0.983, 0.0076796875, 0, 128
256, 0, 0.993, 0.00387890625, 0, 256
512, 0, 0.996, 0.0019453125, 0, 512
1024, 0, 0.996, 0.00097265625, 0, 1024
2048, 0, 1.0, 0.00048828125, 0, 2048
4096, 0, 1.0, 0.000244140625, 0, 4096
8192, 0, 1.0, 0.0001220703125, 0, 8192
16384, 0, 1.0, 6.103515625e-05, 0, 16384
32768, 0, 1.0, 3.0517578125e-05, 0, 32768
65536, 0, 1.0, 1.52587890625e-05, 0, 65536
131072, 0, 1.0, 7.62939453125e-06, 0, 131072
262144, 0, 1.0, 3.814697265625e-06, 0, 262144
524288, 0, 1.0, 1.9073486328125e-06, 0, 524288
1048576, 0, 1.0, 9.5367431640625e-07, 0, 1048576
2097152, 0, 1.0, 4.76837158203125e-07, 0, 2097152
4194304, 0, 1.0, 4.272823178185699e-07, 0, 2340373

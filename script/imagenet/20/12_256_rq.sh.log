# load the base data ../data/imagenet/imagenet_base.fvecs, 
# load the queries ../data/imagenet/imagenet_query.fvecs, 
# load the ground truth ../data/imagenet/20_imagenet_product_groundtruth.ivecs
# ranking metric product
# Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 0,  residual average norm : 0.20509201288223267 max norm: 0.8650335669517517 min norm: 0.028300635516643524
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 1,  residual average norm : 0.1793849766254425 max norm: 0.7998328804969788 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 2,  residual average norm : 0.16378386318683624 max norm: 0.6254310607910156 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 3,  residual average norm : 0.15231886506080627 max norm: 0.5719641447067261 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 4,  residual average norm : 0.14314916729927063 max norm: 0.544162392616272 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 5,  residual average norm : 0.13543079793453217 max norm: 0.48670512437820435 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 6,  residual average norm : 0.12880443036556244 max norm: 0.4589427709579468 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 7,  residual average norm : 0.122958704829216 max norm: 0.4509408175945282 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 8,  residual average norm : 0.11773176491260529 max norm: 0.439401239156723 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 9,  residual average norm : 0.11290118098258972 max norm: 0.43153417110443115 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 10,  residual average norm : 0.10844357311725616 max norm: 0.41485220193862915 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 11,  residual average norm : 0.10439396649599075 max norm: 0.35428881645202637 min norm: 0.0
# compress items
# sorting items
calculate norm.
allocate memory
allocate memory
calculate norm.
allocate memory
allocate memory
calculate norm.
allocate memory
allocate memory
calculate norm.
allocate memory
allocate memory
calculate norm.
allocate memory
allocate memory
# searching!
expected items, overall time, avg recall, avg precision, avg error, avg items
1, 0, 0.04205, 0.841, 0, 1
2, 0, 0.08025, 0.8025, 0, 2
4, 0, 0.15, 0.75, 0, 4
8, 0, 0.26675, 0.666875, 0, 8
16, 0, 0.42875, 0.5359375000000001, 0, 16
32, 0, 0.6090999999999999, 0.3806874999999999, 0, 32
64, 0, 0.7665, 0.23953124999999997, 0, 64
128, 0, 0.8733, 0.136453125, 0, 128
256, 0, 0.93755, 0.07324609375, 0, 256
512, 0, 0.9712999999999999, 0.03794140625, 0, 512
1024, 0, 0.9868500000000001, 0.019274414062500002, 0, 1024
2048, 0, 0.9947999999999999, 0.009714843749999999, 0, 2048
4096, 0, 0.9982000000000001, 0.0048740234375000005, 0, 4096
8192, 0, 0.9993, 0.002439697265625, 0, 8192
16384, 0, 0.99965, 0.0012202758789062501, 0, 16384
32768, 0, 0.9997, 0.00061016845703125, 0, 32768
65536, 0, 0.9998499999999999, 0.0003051300048828125, 0, 65536
131072, 0, 0.99995, 0.00015258026123046874, 0, 131072
262144, 0, 0.99995, 7.629013061523437e-05, 0, 262144
524288, 0, 0.99995, 3.8145065307617185e-05, 0, 524288
1048576, 0, 0.99995, 1.9072532653808593e-05, 0, 1048576
2097152, 0, 0.99995, 9.536266326904296e-06, 0, 2097152
4194304, 0, 0.99995, 4.768133163452148e-06, 0, 4194304

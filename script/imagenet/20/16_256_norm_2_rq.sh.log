# load the base data ../data/imagenet/imagenet_base.fvecs, 
# load the queries ../data/imagenet/imagenet_query.fvecs, 
# load the ground truth ../data/imagenet/20_imagenet_product_groundtruth.ivecs
# ranking metric product
# NormPQ, percentiles: 65536, quantize: Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>Subspace PQ, M: 1, Ks : 256, code_dtype: <class 'numpy.uint8'>
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 0,  residual average norm : 0.6855478286743164 max norm: 1.0495868921279907 min norm: 0.13065466284751892
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 1,  residual average norm : 0.6009670495986938 max norm: 1.0073009729385376 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 2,  residual average norm : 0.5484013557434082 max norm: 0.9586108922958374 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 3,  residual average norm : 0.5102019309997559 max norm: 0.908643901348114 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 4,  residual average norm : 0.47908294200897217 max norm: 0.8663941621780396 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 5,  residual average norm : 0.4528314769268036 max norm: 0.8346607089042664 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 6,  residual average norm : 0.43026167154312134 max norm: 0.8155104517936707 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 7,  residual average norm : 0.410273939371109 max norm: 0.7931651473045349 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 8,  residual average norm : 0.39252129197120667 max norm: 0.7769964337348938 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 9,  residual average norm : 0.37635284662246704 max norm: 0.7596412301063538 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 10,  residual average norm : 0.36141061782836914 max norm: 0.7362945675849915 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 11,  residual average norm : 0.34784430265426636 max norm: 0.7076353430747986 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 12,  residual average norm : 0.33493122458457947 max norm: 0.685365617275238 min norm: 0.0
#    Training the subspace: 0 / 1, 0 -> 150
# layer: 13,  residual average norm : 0.3233566880226135 max norm: 0.667134165763855 min norm: 0.0
# compress items
# sorting items
# searching!
expected items, overall time, avg recall, avg precision, avg error, avg items
1, 0, 0.04395, 0.879, 0, 1
2, 0, 0.0867, 0.867, 0, 2
4, 0, 0.16610000000000003, 0.8305000000000001, 0, 4
8, 0, 0.31085, 0.7771250000000001, 0, 8
16, 0, 0.5287999999999999, 0.6609999999999999, 0, 16
32, 0, 0.7583, 0.4739375, 0, 32
64, 0, 0.9045, 0.28265625, 0, 64
128, 0, 0.9709500000000001, 0.1517109375, 0, 128
256, 0, 0.99245, 0.07753515625, 0, 256
512, 0, 0.998, 0.038984375, 0, 512
1024, 0, 0.9996999999999999, 0.019525390625, 0, 1024
2048, 0, 0.9999000000000001, 0.0097646484375, 0, 2048
4096, 0, 0.9999000000000001, 0.00488232421875, 0, 4096
8192, 0, 0.99995, 0.0024412841796875, 0, 8192
16384, 0, 0.99995, 0.00122064208984375, 0, 16384
32768, 0, 1.0, 0.0006103515625, 0, 32768
65536, 0, 1.0, 0.00030517578125, 0, 65536
131072, 0, 1.0, 0.000152587890625, 0, 131072
262144, 0, 1.0, 7.62939453125e-05, 0, 262144
524288, 0, 1.0, 3.814697265625e-05, 0, 524288
1048576, 0, 1.0, 1.9073486328125e-05, 0, 1048576
2097152, 0, 1.0, 9.5367431640625e-06, 0, 2097152
4194304, 0, 1.0, 4.76837158203125e-06, 0, 4194304